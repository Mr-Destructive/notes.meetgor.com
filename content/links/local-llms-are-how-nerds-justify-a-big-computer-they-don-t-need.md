---
date: 2026-01-02
draft: false
link: https://world.hey.com/dhh/local-llms-are-how-nerds-now-justify-a-big-computer-they-don-t-need-af2fcb7b
preview_description: 'It''s pretty incredible that we''re able to run all these awesome
  AI models on our own hardware now. From downscaled versions of DeepSeek to gpt-oss-20b,
  there are many options for many types of computers. But let''s get real here: they''re
  all vastly behind the frontier models available for rent, and thus for most developers
  a curiosity a...'
preview_image: https://world.hey.com/dhh/avatar-fb368b1ee9b185dc2a09b03eabdb61678dd55244
tags:
- ai
title: Local LLMs are how nerds justify a big computer they don’t need
---

# Local LLMs are how nerds justify a big computer they don’t need

**Link:** https://world.hey.com/dhh/local-llms-are-how-nerds-now-justify-a-big-computer-they-don-t-need-af2fcb7b

## Context

Curiosity gets the better of them. I have a 8GB device, I can barely run a 1B parameter model. I get frustrated but have nothing to complain. I can use ChatGPT in temporory mode, or incognito mode if I don’t want it to attach it to the memory. I don’t see using local models on scale is justifiable just yet.

**Source:** techstructive-weekly-75
