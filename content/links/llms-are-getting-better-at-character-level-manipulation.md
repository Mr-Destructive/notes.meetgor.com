---
title: "LLMs are getting better at character-level manipulation"
date: 2026-01-24
draft: false
---

# LLMs are getting better at character-level manipulation

**Link:** https://blog.burkert.me/posts/llm_evolution_character_manipulation/

## Context

3. [LLMs are getting better at character-level manipulation](https://blog.burkert.me/posts/llm_evolution_character_manipulation/)
    - Its evident from the test that newer and larger models are better at generalizing Base64 encoding and decoding. So that implies they will get better at character-level manipulation and analysis.
    - Sadly the how many râ€™s in strawberry problem will be solvable by LLMs
    - Thinking is out of the equation, the crux here is the tokenisation, the better sense of the word you have, the better it understands, but the fine balance between less and more context is critical, and I think it is still being fine tuned to get a sweet spot.

**Source:** techstructive-weekly-64
