---
title: "Guide to Local LLM Models"
date: 2025-12-26
draft: false
tags: ["ai"]
link: "https://www.aiforswes.com/p/you-dont-need-to-spend-100mo-on-claude"
---

# Guide to Local LLM Models

**Link:** https://www.aiforswes.com/p/you-dont-need-to-spend-100mo-on-claude

## Context

Ok, the VRAM and RAM is somethign is quite critical. If you have less RAM and much VRAM, its no use, you need to have sufficient RAM in order to run a good enough model, VRAM wouldnâ€™t handle it.

**Source:** techstructive-weekly-74
