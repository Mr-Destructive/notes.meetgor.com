---type: newsletter
title: "Techstructive Weekly #38"
date: 2025-04-19T00:00:00
slug: "techstructive-weekly-38"
tags: ["ai", "career", "frontend", "git", "go", "newsletter", "python", "sql"]
---

## Week #38

A week after ages, I finally had some energy, some spark back. I am actually overwhelmed with this AI revolution, too many ideas, too little mental space, its just moving too fast. Like I feel, I was just in this newsletter yesterday, and here I am a week gone with so many things launched, dropped, and bomb shelled.

### Speed in the AI Revolution

Yes, AI launches feel like a bomb shell being dropped, in a good way, but can someone tell these AI companies to take a breath? At this pace, we all will be juggling models all the time. New models every week, deprecation of previous models, new ways to interact with tools, it’s just creating an endless cycle of progress. Yes, we might be progressing in better human intelligence,e but at the cost of letting too many possibilities just dying out due to speed.

Thanks for reading Techstructive Weekly! Subscribe for free to receive new posts and support my work.

Speed is good, but over-speeding or over-of-anything is bad. And the AI revolution is just too speedy, its not letting people take time and create, it’s bombarding with its possibilities. I might be wrong here, you might see very tiny progress being made, yes that is also true, but the amount of content being generated around this and filtering the noise is too hard right now.

Anyways, enough of that big ramble, I might continue this in a separate post altogether where I will distill this thought.

### Personal Update

On the personal update side, I was full of energy this week. Open-AI just partnered with WindSurf to provide the 4.1 models access for free for a week. And I was not going to miss out on that, so I gave it a shot at vibe coding. it was fun to say the least, it did everything (rightly, maybe not completely though). This vibe coding is not bad if you know what you are doing, and at times, at AI fails, you can always steer it.

I also edited and recorded a mammoth 52-minute video, a one-shot voice-over. That brings me joy, from 6 6-minute video taking 3 hours to 1 1-hour video taking just over an hour to add voice over, that is a win for me.

### Vibe Coding Experience

I did some digging in the codex cli, I gave windsurf a shot to create this cli in Python, and it did, but it was not what I expected; it just wrapped the API call in a cli. That also might have taken me a few hours, but AI did that in a few minutes, impressive stuff. It’s not like waiting and staring at it code, but rather, nudge, debug, and steer the conversation to progress. It’s more of you driving it to do what you want to do and let it handle the decisions to reach there, no, how granular you want the autonomy that you can always do, so:

- Lesser autonomy> Use inline completion that Tab tab thing, it’s kind of good at doing that
- More Autonomy> Use the chat interface to talk through the solution to it and let it create. Here, it’s not that great. The more specific you get, the less it grasps. The coherence is a little dicey here
    - Maybe this is good for an initial prototype. It removes that staring at a blank page, you will have something to fix, something to fight the way for

This needs more time and experience to comment on, but I have been doing a similar thing at work, and most of the time it’s me who has given the direction for the LLM to take, and it's helping me find out things and narrow down my search space and effort.

Overall, the week was pretty refreshing and exciting. Learnt a lot of things, which you can read more about.

### Quote of the week

> **"If everything is important, then nothing is."**
> 
> — **Patrick Lencioni**

In this AI hype, everything seems to draw attention, and after attention is all you need.

But is it? Every model seems to be different in some or the other way, however good or bad it may be. In the last month itself, there are so many things launched, revealed, and invented that there is more than a year's worth of discovery to be done, let alone in the past year, we have decades worth of rundown content with us. But it’s just too unstable and fast.

If every AI company thinks their model is important and ground-breaking or SOTA (state of the art) for each of their new models, then I think none of them really are.

Isn’t that part clear to them, or are they just too busy to keep up with the AI Race? The latter seems likely, but who am I to comment on that? I can take my time to absorb the things happening and filter out the things myself from the noise and potential unimportant buzz and hype (maybe important later) out there.

